{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Workshop\n",
    "\n",
    "## Introduction\n",
    "Welcome to this hands-on computer vision workshop! In this session, we'll explore fundamental techniques including:\n",
    "- Unsurpevised Vision with MNIST\n",
    "- Training with YOLOV8\n",
    "- Create your own model for images\n",
    "\n",
    "Remember to activate your environemnt !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO ! You Only Look Once \n",
    "\n",
    "YOLO as others are pre-stablished models.\n",
    "\n",
    "You dont require making a model only stablishing the input and output but at  the cost of high power computational due to the generality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is highly recommend to have everything isntall for the GPU because it is fully needed otherwise use  Google Collab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If using TensorFlow **with GPU**\n",
    "pip install numpy matplotlib tensorflow[and-cuda]==2.15.0 torch==2.2.0+cu118 torchvision==0.17.0+cu118 torchaudio==2.2.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "⚠️ *Make sure your CUDA/cuDNN versions match TensorFlow and PyTorch requirements (CUDA 11.8 recommended).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.0+cu118\n",
      "torchvision version: 0.17.0+cu118\n",
      "CUDA disponible: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highy recommend always make does print to ensure everything is in order and well installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Esto devuelve True si hay GPU con CUDA\n",
    "print(torch.cuda.device_count())  # Número de GPUs detectadas\n",
    "print(torch.cuda.get_device_name(0))  # Nombre de la primera GPU (si existe)\n",
    "print(torch.version.cuda)  # Versión de CUDA que PyTorch está usando\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model YOLOV8 \n",
    "\n",
    "There are several version of yolov8 models\n",
    "- yolov8n is the nano version \n",
    "- yolov8m is the medium version \n",
    "- yolov8s is the small version \n",
    "\n",
    "The batch is depending of the capacity of your GPU in this case 4gb is very capable of batch:16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.162 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.160  Python-3.10.18 torch-2.2.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=day2_files\\dataset.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train14, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train14, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.60.3 ms, read: 13.52.7 MB/s, size: 192.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\yourk\\Documents\\3.Universitat\\Taller_Ingenieria\\day2_files\\labels\\Train... 749 images, 0 backgrounds, 0 corrupt: 100%|██████████| 749/749 [00:03<00:00, 208.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\yourk\\Documents\\3.Universitat\\Taller_Ingenieria\\day2_files\\labels\\Train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.80.3 ms, read: 8.02.6 MB/s, size: 217.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\yourk\\Documents\\3.Universitat\\Taller_Ingenieria\\day2_files\\labels\\Val... 200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 200/200 [00:01<00:00, 148.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\yourk\\Documents\\3.Universitat\\Taller_Ingenieria\\day2_files\\labels\\Val.cache\n",
      "Plotting labels to runs\\detect\\train14\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train14\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100      2.14G     0.9781      1.677      1.137         68        640: 100%|██████████| 47/47 [00:18<00:00,  2.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:05<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.953      0.287      0.473      0.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/100      2.13G      1.174      1.389      1.257         91        640: 100%|██████████| 47/47 [00:14<00:00,  3.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.477      0.269      0.306      0.157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/100      2.16G      1.271      1.459      1.306         53        640: 100%|██████████| 47/47 [00:20<00:00,  2.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.41      0.411      0.341      0.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/100      2.11G      1.332      1.471      1.338        119        640: 100%|██████████| 47/47 [00:15<00:00,  3.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.534      0.453      0.469      0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/100      2.12G      1.331      1.449      1.347        114        640: 100%|██████████| 47/47 [00:13<00:00,  3.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.698       0.52      0.637      0.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/100      2.09G      1.271      1.368      1.317         72        640: 100%|██████████| 47/47 [00:12<00:00,  3.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.703      0.507      0.598      0.356\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/100      2.18G      1.242      1.266      1.306         67        640: 100%|██████████| 47/47 [00:11<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.73      0.611      0.696      0.426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      8/100      2.11G      1.255      1.293      1.317         81        640: 100%|██████████| 47/47 [00:13<00:00,  3.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.674      0.605      0.679       0.41\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      9/100      2.11G      1.243      1.247      1.301        103        640: 100%|██████████| 47/47 [00:12<00:00,  3.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.741        0.6      0.711      0.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     10/100      2.09G      1.197      1.221      1.276         81        640: 100%|██████████| 47/47 [00:12<00:00,  3.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.739      0.612      0.714       0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     11/100      2.18G        1.2      1.233      1.287         95        640: 100%|██████████| 47/47 [00:11<00:00,  4.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.748      0.603      0.709      0.445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     12/100      2.12G      1.195      1.207      1.271        116        640: 100%|██████████| 47/47 [00:13<00:00,  3.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.71      0.657      0.734      0.453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     13/100      2.14G       1.18      1.125      1.254         95        640: 100%|██████████| 47/47 [00:12<00:00,  3.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.67      0.547      0.605      0.374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     14/100      2.07G      1.137      1.132      1.242         83        640: 100%|██████████| 47/47 [00:11<00:00,  3.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.749      0.673      0.754      0.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     15/100      2.11G      1.127      1.078       1.24         78        640: 100%|██████████| 47/47 [00:13<00:00,  3.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.701      0.638      0.726      0.448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     16/100      2.11G      1.089      1.052      1.214         74        640: 100%|██████████| 47/47 [00:12<00:00,  3.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.723      0.668      0.744      0.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     17/100      2.16G      1.099      1.071       1.22        105        640: 100%|██████████| 47/47 [00:12<00:00,  3.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.741      0.655      0.741      0.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     18/100      2.12G      1.086       1.06      1.217         60        640: 100%|██████████| 47/47 [00:12<00:00,  3.64it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.735      0.638      0.731      0.485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     19/100      2.11G      1.088      1.046      1.212         72        640: 100%|██████████| 47/47 [00:12<00:00,  3.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.744      0.632      0.738      0.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     20/100      2.12G       1.05       1.02      1.201        108        640: 100%|██████████| 47/47 [00:13<00:00,  3.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.699       0.64      0.723       0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     21/100      2.15G      1.066     0.9911      1.186        104        640: 100%|██████████| 47/47 [00:15<00:00,  3.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.819      0.675        0.8       0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     22/100      2.12G      1.024     0.9615       1.17        119        640: 100%|██████████| 47/47 [00:14<00:00,  3.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.773      0.655      0.776      0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     23/100       2.1G      1.023     0.9584      1.166         81        640: 100%|██████████| 47/47 [00:12<00:00,  3.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.787      0.672      0.775      0.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     24/100       2.1G      1.039     0.9557      1.185         66        640: 100%|██████████| 47/47 [00:14<00:00,  3.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.738      0.685      0.749      0.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     25/100       2.1G      1.001     0.9215      1.169         92        640: 100%|██████████| 47/47 [00:12<00:00,  3.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.746      0.668      0.762      0.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     26/100      2.08G     0.9933     0.9334      1.157         45        640: 100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.788      0.688      0.775      0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     27/100      2.11G      1.001     0.9266      1.157         77        640: 100%|██████████| 47/47 [00:11<00:00,  4.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.794      0.691      0.783      0.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     28/100      2.11G      1.006     0.9255      1.151        118        640: 100%|██████████| 47/47 [00:12<00:00,  3.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769        0.8      0.706      0.791      0.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     29/100      2.14G     0.9629     0.9185      1.145        111        640: 100%|██████████| 47/47 [00:11<00:00,  4.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.793      0.709      0.805      0.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     30/100      2.07G     0.9651     0.8909      1.142         42        640: 100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.831      0.714      0.813      0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     31/100       2.1G     0.9601     0.9091      1.155         73        640: 100%|██████████| 47/47 [00:11<00:00,  4.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.796      0.691      0.796      0.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     32/100      2.11G     0.9995     0.9027      1.149        124        640: 100%|██████████| 47/47 [00:11<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.825      0.722      0.821      0.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     33/100      2.11G     0.9848     0.8792      1.142        103        640: 100%|██████████| 47/47 [00:12<00:00,  3.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.82      0.714      0.821      0.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     34/100      2.09G     0.9453     0.8763      1.137        108        640: 100%|██████████| 47/47 [00:12<00:00,  3.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.792      0.693      0.782      0.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     35/100      2.16G     0.9418       0.85      1.118         93        640: 100%|██████████| 47/47 [00:11<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.803      0.749      0.822      0.578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     36/100       2.1G     0.9604     0.8749      1.145         50        640: 100%|██████████| 47/47 [00:12<00:00,  3.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.788      0.754      0.822      0.566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     37/100      2.08G     0.9346     0.8491      1.118         95        640: 100%|██████████| 47/47 [00:11<00:00,  4.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.83      0.686      0.799      0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     38/100      2.08G     0.9284      0.829      1.113        115        640: 100%|██████████| 47/47 [00:11<00:00,  3.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.822      0.701      0.806      0.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     39/100       2.1G     0.9191     0.8228      1.103         90        640: 100%|██████████| 47/47 [00:11<00:00,  3.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.824      0.717      0.824      0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     40/100      2.11G     0.8909     0.7827      1.087        132        640: 100%|██████████| 47/47 [00:12<00:00,  3.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.801      0.687      0.796       0.55\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     41/100      2.17G     0.9007     0.7993      1.099         91        640: 100%|██████████| 47/47 [00:11<00:00,  3.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.821      0.694      0.807       0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     42/100      2.14G      0.881     0.8051      1.096        110        640: 100%|██████████| 47/47 [00:11<00:00,  3.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.821      0.709      0.821      0.594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     43/100      2.12G     0.8946     0.7874      1.087         71        640: 100%|██████████| 47/47 [00:11<00:00,  4.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.78      0.697       0.79      0.566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     44/100      2.17G     0.9077     0.8077      1.105         83        640: 100%|██████████| 47/47 [00:11<00:00,  3.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.836      0.704       0.82        0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     45/100      2.14G     0.8614     0.7623      1.079         55        640: 100%|██████████| 47/47 [00:11<00:00,  3.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.85      0.714      0.824      0.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     46/100      2.23G     0.8786      0.779      1.088         82        640: 100%|██████████| 47/47 [00:11<00:00,  3.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.821      0.705      0.809      0.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     47/100      2.12G     0.8606     0.7545       1.08         57        640: 100%|██████████| 47/47 [00:11<00:00,  3.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.82      0.728      0.829        0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     48/100      2.12G     0.8476     0.7348      1.068         67        640: 100%|██████████| 47/47 [00:11<00:00,  3.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.815       0.73      0.822      0.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     49/100       2.1G     0.8606     0.7476      1.082         87        640: 100%|██████████| 47/47 [00:11<00:00,  3.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.868      0.728      0.837        0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     50/100      2.15G     0.8491      0.741       1.07         87        640: 100%|██████████| 47/47 [00:12<00:00,  3.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.833      0.745      0.837      0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     51/100      2.05G     0.8221     0.7196      1.063         69        640: 100%|██████████| 47/47 [00:12<00:00,  3.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.812      0.741      0.826      0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     52/100      2.12G     0.8378     0.7231       1.07         75        640: 100%|██████████| 47/47 [00:12<00:00,  3.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.844      0.702      0.828      0.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     53/100       2.1G     0.8596     0.7379      1.074         75        640: 100%|██████████| 47/47 [00:11<00:00,  4.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.812      0.769      0.836      0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     54/100      2.07G     0.8377     0.7294      1.054         72        640: 100%|██████████| 47/47 [00:11<00:00,  4.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.853      0.706      0.829      0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     55/100      2.12G     0.8318     0.7013      1.047        142        640: 100%|██████████| 47/47 [00:12<00:00,  3.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.828      0.724      0.833      0.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     56/100      2.11G     0.8253     0.6925       1.06         64        640: 100%|██████████| 47/47 [00:13<00:00,  3.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.832      0.735      0.837      0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     57/100       2.1G     0.8262     0.7061      1.072         88        640: 100%|██████████| 47/47 [00:13<00:00,  3.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.857      0.733      0.847      0.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     58/100      2.14G     0.8324     0.6949      1.058         81        640: 100%|██████████| 47/47 [00:11<00:00,  3.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.871      0.724      0.832      0.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     59/100      2.12G     0.8187     0.7007      1.053         66        640: 100%|██████████| 47/47 [00:12<00:00,  3.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.862      0.736      0.838      0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     60/100      2.12G     0.8153     0.6923      1.052         89        640: 100%|██████████| 47/47 [00:11<00:00,  3.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.854      0.718      0.831      0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     61/100       2.1G     0.7889      0.677      1.037         68        640: 100%|██████████| 47/47 [00:12<00:00,  3.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.812      0.753      0.834      0.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     62/100       2.1G     0.7859     0.6527      1.039         92        640: 100%|██████████| 47/47 [00:12<00:00,  3.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.819      0.749      0.836      0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     63/100      2.11G     0.7805     0.6565      1.038         72        640: 100%|██████████| 47/47 [00:11<00:00,  4.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.783      0.732      0.811       0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     64/100      2.11G      0.752     0.6271      1.021        116        640: 100%|██████████| 47/47 [00:15<00:00,  3.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.86      0.719      0.827      0.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     65/100       2.1G     0.7825     0.6508      1.042         68        640: 100%|██████████| 47/47 [00:13<00:00,  3.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.849      0.735      0.838      0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     66/100      2.17G     0.7622     0.6356      1.032         80        640: 100%|██████████| 47/47 [00:13<00:00,  3.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.881      0.692       0.84       0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     67/100      2.11G     0.7694     0.6417      1.028         76        640: 100%|██████████| 47/47 [00:13<00:00,  3.41it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.848      0.717      0.839       0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     68/100      2.11G     0.7609     0.6428      1.027        103        640: 100%|██████████| 47/47 [00:13<00:00,  3.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.857      0.744      0.846      0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     69/100      2.16G     0.7626     0.6362      1.026         90        640: 100%|██████████| 47/47 [00:13<00:00,  3.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.826       0.76      0.842      0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     70/100       2.1G     0.7563     0.6291      1.022        103        640: 100%|██████████| 47/47 [00:14<00:00,  3.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.853      0.731      0.837      0.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     71/100       2.1G     0.7534     0.6198      1.013         88        640: 100%|██████████| 47/47 [00:13<00:00,  3.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.824      0.752       0.84      0.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     72/100       2.1G     0.7428     0.6231      1.014         71        640: 100%|██████████| 47/47 [00:14<00:00,  3.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.85      0.738      0.841      0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     73/100      2.12G     0.7253     0.6102       1.01        119        640: 100%|██████████| 47/47 [00:14<00:00,  3.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.862      0.746      0.849       0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     74/100      2.06G     0.7205     0.5997      1.004         84        640: 100%|██████████| 47/47 [00:13<00:00,  3.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.852      0.736      0.847       0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     75/100      2.18G      0.746     0.6061      1.008         66        640: 100%|██████████| 47/47 [00:13<00:00,  3.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.835      0.761      0.844      0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     76/100      2.12G     0.7418     0.6103      1.014         61        640: 100%|██████████| 47/47 [00:12<00:00,  3.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.821      0.759      0.839      0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     77/100      2.12G      0.733     0.5924      1.001         94        640: 100%|██████████| 47/47 [00:11<00:00,  4.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.846      0.744      0.842      0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     78/100      2.08G     0.7157     0.5881     0.9988         83        640: 100%|██████████| 47/47 [00:11<00:00,  4.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.857      0.753      0.856       0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     79/100       2.1G     0.7151     0.5898      1.009         69        640: 100%|██████████| 47/47 [00:11<00:00,  4.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.83      0.765      0.848      0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     80/100       2.1G     0.7205     0.5945      1.004         85        640: 100%|██████████| 47/47 [00:11<00:00,  4.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.776      0.795      0.851      0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     81/100      2.07G     0.6916     0.5751      1.001         89        640: 100%|██████████| 47/47 [00:11<00:00,  4.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.848      0.746      0.852      0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     82/100      2.07G        0.7     0.5666     0.9891         90        640: 100%|██████████| 47/47 [00:11<00:00,  4.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.855      0.728      0.846      0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     83/100      2.07G     0.7173     0.5776      1.004        124        640: 100%|██████████| 47/47 [00:11<00:00,  4.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.852      0.746      0.848      0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     84/100      2.12G     0.6976     0.5628     0.9909         90        640: 100%|██████████| 47/47 [00:11<00:00,  4.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.828      0.763       0.85       0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     85/100       2.1G      0.697     0.5731     0.9964         96        640: 100%|██████████| 47/47 [00:11<00:00,  4.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.848      0.739      0.841      0.641\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     86/100      2.14G     0.6988     0.5552     0.9897         86        640: 100%|██████████| 47/47 [00:11<00:00,  4.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.87      0.735      0.853      0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     87/100      2.11G     0.6928     0.5585     0.9928         83        640: 100%|██████████| 47/47 [00:11<00:00,  4.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.826      0.774       0.85      0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     88/100      2.12G     0.6725     0.5413     0.9807        109        640: 100%|██████████| 47/47 [00:11<00:00,  4.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769       0.84      0.754      0.853      0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     89/100      2.11G     0.6797     0.5473     0.9744         94        640: 100%|██████████| 47/47 [00:11<00:00,  4.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.885      0.719      0.847      0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     90/100      2.07G     0.6715      0.554      0.986         61        640: 100%|██████████| 47/47 [00:11<00:00,  4.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.857      0.744       0.85      0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     91/100      2.04G     0.5954     0.4755      0.923         52        640: 100%|██████████| 47/47 [00:11<00:00,  4.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.851      0.746       0.84      0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     92/100      2.05G     0.5747     0.4373     0.9085         42        640: 100%|██████████| 47/47 [00:11<00:00,  4.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.853      0.749      0.846      0.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     93/100      2.05G     0.5643     0.4303     0.9022         48        640: 100%|██████████| 47/47 [00:11<00:00,  4.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.841      0.744       0.84      0.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     94/100      2.06G     0.5627     0.4301      0.907         38        640: 100%|██████████| 47/47 [00:11<00:00,  4.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.852      0.744      0.848      0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     95/100      2.04G     0.5656     0.4219     0.9093         32        640: 100%|██████████| 47/47 [00:11<00:00,  4.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.813      0.782      0.854      0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     96/100      2.05G     0.5468     0.4151     0.8998         45        640: 100%|██████████| 47/47 [00:11<00:00,  4.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.814      0.763       0.85      0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     97/100      2.05G      0.542     0.4164     0.8942         46        640: 100%|██████████| 47/47 [00:11<00:00,  4.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.868      0.736      0.854      0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     98/100      2.06G     0.5421      0.408      0.894         39        640: 100%|██████████| 47/47 [00:11<00:00,  4.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.877      0.739      0.857      0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     99/100      2.04G     0.5391     0.3994     0.8971         48        640: 100%|██████████| 47/47 [00:11<00:00,  3.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.835      0.779      0.861      0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    100/100      2.05G     0.5201     0.4005     0.8941         56        640: 100%|██████████| 47/47 [00:12<00:00,  3.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.884      0.735      0.859      0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100 epochs completed in 0.474 hours.\n",
      "Optimizer stripped from runs\\detect\\train14\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train14\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train14\\weights\\best.pt...\n",
      "Ultralytics 8.3.160  Python-3.10.18 torch-2.2.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        200        769      0.884      0.735      0.859      0.654\n",
      "Speed: 0.4ms preprocess, 3.0ms inference, 0.0ms loss, 4.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train14\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000002430525C700>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,     0.99342,     0.99342,\n",
       "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,\n",
       "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,\n",
       "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,\n",
       "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,\n",
       "            0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,     0.99342,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,\n",
       "             0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,\n",
       "             0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,\n",
       "             0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,\n",
       "             0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,\n",
       "             0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9917,      0.9878,      0.9878,      0.9878,      0.9878,      0.9878,     0.98456,     0.98456,     0.98456,     0.98456,     0.98456,     0.98456,\n",
       "            0.98456,     0.98456,     0.98456,     0.98456,     0.98456,     0.98456,     0.98456,     0.98456,     0.98456,     0.98456,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,\n",
       "            0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,\n",
       "            0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,\n",
       "            0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98433,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,\n",
       "            0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,     0.98256,\n",
       "            0.98256,     0.98256,     0.98256,     0.97989,     0.97989,     0.97989,     0.96927,     0.96927,     0.96927,     0.96927,     0.96927,     0.96927,     0.96927,     0.96927,     0.96676,     0.96676,     0.96676,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,\n",
       "            0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96579,     0.96438,     0.96438,     0.96438,     0.96438,     0.96438,     0.96438,\n",
       "            0.96438,     0.96438,     0.96438,     0.96438,     0.96438,     0.96438,     0.96438,     0.96438,     0.96438,     0.96438,     0.96287,     0.96287,     0.96287,     0.96287,     0.96287,     0.96287,     0.96287,     0.96287,     0.96287,     0.96287,     0.96287,     0.96287,     0.96287,\n",
       "            0.96069,     0.96069,     0.95884,     0.95884,     0.95884,     0.95884,     0.95884,     0.95884,     0.95884,     0.95694,     0.95694,     0.95694,     0.95694,     0.95694,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,\n",
       "             0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,\n",
       "             0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,      0.9569,\n",
       "             0.9569,      0.9569,     0.95329,     0.95329,     0.95329,     0.95329,     0.95329,     0.95329,     0.95329,     0.95137,      0.9499,      0.9499,      0.9499,      0.9499,      0.9499,      0.9499,      0.9499,     0.94802,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,\n",
       "            0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,\n",
       "            0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94788,     0.94656,     0.94656,     0.94656,     0.94656,     0.94656,     0.94656,\n",
       "            0.94656,     0.94361,     0.94361,     0.94361,     0.94361,     0.94361,     0.94361,     0.94361,     0.94361,     0.94206,     0.94206,     0.93934,     0.93934,     0.93934,     0.93934,     0.93934,     0.93934,     0.93934,     0.93934,     0.93934,     0.93636,     0.93636,     0.93636,\n",
       "            0.93636,     0.93636,     0.93636,     0.93309,     0.93153,     0.92857,     0.92857,     0.92857,     0.92857,     0.92756,     0.92756,     0.92756,     0.92756,     0.92756,     0.92756,     0.92756,     0.91972,     0.91972,     0.91854,     0.91854,     0.91854,     0.91854,     0.91566,\n",
       "            0.91566,     0.91566,     0.91438,     0.91438,     0.91297,     0.91297,     0.91032,     0.91032,     0.91032,     0.90894,     0.90894,     0.90756,     0.90635,     0.90635,     0.90635,     0.90216,     0.90216,     0.90099,     0.90099,     0.90099,     0.90065,     0.90065,     0.90065,\n",
       "            0.90065,     0.90065,     0.90065,     0.90065,     0.90065,     0.90065,     0.89951,     0.89951,      0.8955,      0.8955,      0.8955,     0.89423,     0.89297,     0.89297,     0.88942,     0.88942,     0.88942,     0.88942,     0.88942,     0.88679,     0.88474,     0.88474,     0.88474,\n",
       "            0.88474,     0.88474,     0.88217,     0.88217,     0.87711,     0.87711,     0.87615,     0.87615,     0.87615,       0.875,     0.86989,     0.86099,     0.86099,     0.85377,     0.85377,     0.85022,     0.85022,     0.84694,     0.84694,     0.84493,     0.84493,     0.84493,     0.84271,\n",
       "            0.83714,     0.83714,     0.83714,     0.83381,      0.8331,      0.8331,      0.8331,     0.83099,     0.83006,     0.82913,     0.82913,     0.82754,     0.82754,     0.82754,     0.82459,     0.82459,     0.82459,     0.82369,     0.82167,     0.82167,     0.81967,     0.81905,     0.81905,\n",
       "            0.81905,     0.81486,     0.81402,     0.81317,     0.81283,     0.81283,     0.81283,     0.81283,     0.80927,     0.80927,     0.80927,     0.80927,     0.80815,     0.80815,     0.80815,     0.80815,     0.80815,     0.80734,     0.80734,      0.8013,     0.79845,     0.79768,     0.79768,\n",
       "            0.79589,     0.79135,     0.79135,     0.79135,     0.78861,     0.78589,     0.77861,     0.77861,     0.77861,     0.77312,     0.76492,     0.76058,     0.76058,     0.75812,     0.75569,        0.75,        0.75,        0.75,     0.74941,     0.74356,     0.74299,     0.74299,      0.7407,\n",
       "            0.73249,     0.72203,     0.72203,     0.71508,     0.71413,     0.71413,     0.71413,     0.71286,     0.71082,     0.70646,     0.70217,     0.70217,      0.6971,      0.6971,      0.6971,      0.6971,     0.69296,     0.69182,     0.68995,     0.68995,     0.68882,     0.68553,     0.67526,\n",
       "            0.66871,     0.66871,      0.6623,     0.65998,     0.65768,     0.65768,     0.65025,     0.64425,     0.63776,     0.63324,     0.63324,     0.63118,     0.63093,     0.63009,     0.63009,     0.61362,     0.61228,     0.60434,     0.60434,     0.59875,     0.58603,     0.56661,     0.56224,\n",
       "            0.56224,     0.55841,     0.55693,     0.55638,     0.55638,      0.5522,     0.54943,     0.54104,     0.54104,     0.54097,     0.53037,     0.52786,     0.52696,     0.52696,     0.52696,     0.52652,     0.52527,     0.52527,     0.52045,     0.51114,     0.49391,     0.49391,     0.48218,\n",
       "            0.47721,      0.4756,     0.47466,     0.47466,     0.47051,     0.46801,     0.46774,     0.46774,      0.4619,     0.45921,     0.44836,     0.44836,     0.44501,     0.43192,     0.42086,      0.4128,      0.4128,     0.39264,     0.38929,     0.38727,     0.38727,     0.38676,      0.3758,\n",
       "            0.37375,     0.37375,     0.36845,     0.36725,     0.35761,     0.35739,     0.35739,     0.34812,     0.34193,     0.34193,     0.34193,     0.33901,     0.33042,     0.32563,     0.32563,     0.31236,     0.31078,     0.30451,     0.29765,     0.29765,     0.29479,     0.28331,     0.27584,\n",
       "            0.27584,     0.27362,     0.27317,     0.26606,     0.26606,     0.24285,     0.24086,     0.23844,     0.23486,     0.23486,     0.21025,     0.20468,     0.20065,     0.20065,     0.18825,     0.18441,     0.17684,     0.17684,     0.16984,     0.16443,     0.15998,     0.15554,      0.1511,\n",
       "            0.14665,     0.14221,     0.13776,     0.13332,     0.12888,     0.12443,     0.11999,     0.11554,      0.1111,     0.10666,     0.10221,    0.097768,    0.093324,     0.08888,    0.084436,    0.079992,    0.075548,    0.071104,     0.06666,    0.062216,    0.057772,    0.053328,    0.048884,\n",
       "            0.04444,    0.039996,    0.035552,    0.031108,    0.026664,     0.02222,    0.017776,    0.013332,    0.008888,    0.004444,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.28522,     0.28522,     0.37122,     0.41826,     0.45102,     0.47889,     0.49931,     0.51549,     0.53012,     0.54441,     0.55144,     0.56304,     0.57145,      0.5781,     0.58517,     0.59448,     0.60196,     0.60954,     0.61455,     0.61835,     0.62303,     0.62719,     0.63216,\n",
       "            0.63666,     0.64014,     0.64528,     0.65033,     0.65372,     0.65768,     0.65929,     0.66153,     0.66118,     0.66405,     0.66752,     0.66978,     0.67167,     0.67507,     0.67664,     0.67882,     0.68064,       0.681,     0.68197,     0.68336,     0.68504,     0.68726,     0.68775,\n",
       "            0.69107,     0.69326,     0.69482,     0.69897,     0.70084,     0.70334,     0.70508,     0.70666,     0.70945,     0.71012,     0.71209,     0.71279,     0.71437,     0.71503,     0.71625,     0.71825,      0.7181,      0.7197,      0.7218,     0.72333,     0.72577,     0.72726,     0.72868,\n",
       "             0.7292,     0.72941,     0.72901,     0.72988,     0.72982,     0.73148,     0.73256,     0.73439,     0.73538,     0.73568,     0.73611,     0.73703,     0.73928,     0.73921,     0.74025,     0.74159,     0.74364,     0.74364,     0.74403,     0.74487,     0.74487,     0.74532,     0.74674,\n",
       "            0.74725,     0.74752,     0.74777,     0.74852,      0.7492,     0.75075,     0.75203,     0.75329,     0.75295,     0.75479,     0.75563,     0.75633,     0.75749,     0.75779,     0.75817,     0.75884,     0.75846,      0.7595,      0.7601,     0.76031,     0.76047,     0.76062,     0.76059,\n",
       "            0.76014,     0.76088,     0.76145,     0.76089,     0.76126,     0.76197,     0.76328,     0.76281,     0.76154,     0.76242,      0.7636,     0.76421,     0.76448,     0.76471,     0.76533,     0.76628,     0.76686,     0.76652,     0.76697,     0.76742,     0.76884,     0.76935,     0.76946,\n",
       "            0.76919,     0.76893,     0.76968,     0.76928,      0.7687,     0.76854,      0.7701,     0.77081,     0.77145,       0.772,     0.77239,     0.77265,     0.77239,     0.77214,     0.77206,     0.77267,     0.77362,     0.77484,     0.77593,     0.77711,     0.77734,     0.77757,     0.77767,\n",
       "            0.77766,     0.77875,     0.77919,     0.77984,     0.78005,     0.78112,     0.78134,     0.78156,      0.7816,      0.7826,     0.78206,     0.78234,     0.78258,     0.78382,     0.78438,     0.78466,     0.78488,      0.7851,     0.78477,     0.78441,     0.78462,     0.78483,     0.78387,\n",
       "            0.78356,     0.78422,     0.78486,     0.78499,     0.78511,     0.78524,     0.78553,     0.78606,     0.78673,     0.78663,     0.78646,     0.78629,     0.78612,     0.78667,     0.78707,     0.78692,     0.78744,     0.78805,     0.78811,     0.78851,     0.78933,      0.7898,     0.78923,\n",
       "            0.78955,     0.79043,     0.79195,     0.79272,     0.79309,     0.79346,      0.7935,     0.79311,     0.79309,     0.79394,     0.79409,     0.79424,     0.79439,     0.79464,      0.7949,     0.79571,     0.79496,     0.79459,     0.79449,     0.79468,     0.79487,      0.7951,     0.79535,\n",
       "            0.79592,      0.7963,     0.79689,     0.79772,     0.79827,     0.79774,     0.79809,     0.79837,     0.79862,     0.79892,     0.79857,     0.79967,     0.79997,     0.79944,     0.79907,     0.79925,     0.79943,     0.79972,     0.80017,     0.80065,     0.80087,     0.80089,     0.80036,\n",
       "            0.80083,     0.80093,     0.80095,     0.80032,     0.80075,     0.80128,     0.80176,     0.80152,     0.80127,     0.80104,     0.80112,      0.8012,     0.80127,     0.80135,     0.80143,      0.8015,     0.80159,     0.80174,     0.80188,     0.80202,     0.80273,     0.80294,     0.80322,\n",
       "            0.80394,     0.80364,     0.80389,      0.8031,     0.80293,     0.80276,     0.80259,     0.80242,     0.80217,     0.80184,     0.80082,     0.80103,     0.80125,     0.80153,     0.80183,      0.8013,     0.80017,     0.80001,     0.79985,     0.79969,     0.79953,     0.79995,     0.80029,\n",
       "            0.80112,     0.80143,     0.80055,     0.80009,     0.79974,     0.79941,     0.79932,     0.79963,     0.79937,     0.79903,     0.79946,     0.79894,     0.79879,     0.79901,     0.79917,      0.7987,     0.79868,     0.79914,     0.79945,     0.80025,     0.79996,     0.79962,     0.79915,\n",
       "            0.79905,     0.79927,     0.79938,      0.7987,     0.79891,     0.79913,     0.79958,       0.799,     0.79909,     0.79918,     0.79927,     0.79936,     0.79945,     0.79964,     0.79984,     0.79971,     0.79951,     0.79854,     0.79839,     0.79882,     0.79914,     0.79936,     0.79956,\n",
       "             0.7997,      0.7995,     0.79929,     0.79908,     0.79884,     0.79847,     0.79732,     0.79763,     0.79788,       0.798,     0.79811,     0.79823,     0.79835,     0.79762,      0.7981,     0.79781,     0.79751,     0.79736,     0.79756,     0.79776,     0.79737,     0.79777,     0.79808,\n",
       "            0.79732,     0.79684,     0.79667,     0.79649,     0.79631,     0.79644,     0.79707,      0.7973,     0.79681,      0.7965,     0.79619,     0.79726,     0.79801,     0.79822,     0.79884,     0.79861,     0.79839,     0.79816,     0.79826,     0.79854,     0.79902,     0.79907,     0.79893,\n",
       "            0.79878,     0.79863,     0.79849,     0.79823,     0.79774,      0.7983,     0.79845,     0.79802,     0.79757,     0.79709,     0.79708,     0.79722,     0.79736,      0.7975,      0.7977,      0.7979,     0.79817,     0.79853,     0.79811,     0.79794,     0.79837,     0.79847,     0.79857,\n",
       "            0.79867,     0.79876,     0.79886,     0.79908,     0.79937,     0.79884,     0.79798,     0.79822,     0.79879,     0.79901,     0.79917,     0.79932,     0.79947,     0.79966,     0.79985,     0.80004,     0.80023,     0.80042,     0.80061,      0.8008,     0.80099,     0.80057,     0.80102,\n",
       "            0.80131,     0.80179,     0.80216,     0.80245,     0.80295,     0.80353,      0.8041,     0.80348,     0.80386,       0.805,     0.80531,      0.8056,     0.80479,     0.80489,     0.80501,     0.80512,     0.80524,     0.80532,     0.80509,     0.80486,     0.80463,     0.80372,     0.80387,\n",
       "            0.80401,     0.80416,     0.80375,     0.80261,     0.80272,     0.80284,     0.80295,     0.80307,     0.80321,     0.80335,      0.8035,     0.80364,     0.80413,     0.80472,     0.80478,     0.80507,     0.80472,     0.80428,     0.80259,     0.80182,     0.80194,     0.80205,     0.80217,\n",
       "             0.8023,     0.80259,     0.80203,     0.80233,     0.80309,     0.80279,      0.8025,     0.80135,     0.80106,     0.80077,     0.80028,     0.79982,     0.80011,      0.8011,     0.80106,     0.80061,      0.8007,     0.80078,     0.80086,     0.80095,     0.80103,     0.80112,      0.8006,\n",
       "            0.80036,     0.80048,      0.8006,     0.80072,     0.80084,     0.80064,     0.80034,     0.80005,     0.79993,     0.79985,     0.79977,     0.79969,     0.79961,     0.79953,     0.79945,     0.79937,     0.79929,     0.79921,     0.79978,     0.80033,     0.80062,      0.8007,     0.79982,\n",
       "            0.79961,     0.79916,     0.79872,     0.79827,      0.7979,     0.79768,     0.79745,     0.79723,     0.79585,     0.79515,      0.7947,     0.79368,     0.79383,     0.79397,     0.79412,     0.79364,     0.79274,     0.79281,     0.79152,     0.79145,     0.79174,     0.79284,     0.79263,\n",
       "            0.79218,     0.79172,     0.79127,      0.7914,     0.79094,     0.79127,     0.79135,     0.79104,     0.79074,     0.79075,     0.79105,     0.78887,     0.78886,     0.78927,     0.78957,     0.78905,     0.78919,     0.78919,     0.78896,     0.78873,      0.7885,     0.78821,      0.7879,\n",
       "             0.7876,     0.78765,     0.78774,     0.78783,     0.78792,     0.78801,     0.78811,     0.78721,     0.78714,     0.78707,       0.787,     0.78693,     0.78686,     0.78679,     0.78671,     0.78664,     0.78657,      0.7865,     0.78643,     0.78636,     0.78687,     0.78729,     0.78742,\n",
       "            0.78719,     0.78696,     0.78673,     0.78645,     0.78615,     0.78584,     0.78553,     0.78522,     0.78491,     0.78488,     0.78499,      0.7851,     0.78521,     0.78532,     0.78511,     0.78418,     0.78369,     0.78399,     0.78465,     0.78489,     0.78504,     0.78519,     0.78535,\n",
       "            0.78593,     0.78607,      0.7862,     0.78634,     0.78647,     0.78591,     0.78529,     0.78483,     0.78436,     0.78389,     0.78306,     0.78199,      0.7821,     0.78221,     0.78232,     0.78243,     0.78254,     0.78166,     0.78072,     0.78038,     0.78068,     0.78091,     0.77982,\n",
       "            0.78042,     0.78063,     0.78083,     0.78078,     0.78061,     0.78043,     0.78026,     0.78009,     0.77985,     0.77947,     0.77909,     0.77822,     0.77727,     0.77736,     0.77746,     0.77755,     0.77764,     0.77774,     0.77835,     0.77803,     0.77771,     0.77731,     0.77526,\n",
       "            0.77338,      0.7729,     0.77226,      0.7726,     0.77284,     0.77187,     0.77122,     0.77129,      0.7715,      0.7717,     0.77154,     0.77134,     0.77115,     0.77096,     0.77073,     0.76976,      0.7687,     0.76734,      0.7668,     0.76652,     0.76624,     0.76707,     0.76642,\n",
       "            0.76589,      0.7654,     0.76478,     0.76418,     0.76369,     0.76258,     0.76298,     0.76114,        0.76,     0.75971,     0.75943,     0.75912,     0.75806,     0.75693,     0.75644,     0.75604,     0.75571,     0.75538,     0.75415,     0.75393,      0.7537,     0.75348,      0.7532,\n",
       "             0.7528,      0.7524,     0.75099,     0.74914,     0.74847,     0.74681,     0.74597,     0.74496,     0.74412,     0.74335,     0.74284,     0.74267,     0.74195,     0.74076,      0.7394,     0.73799,     0.73747,     0.73691,     0.73623,     0.73554,     0.73237,     0.73118,     0.73074,\n",
       "             0.7304,     0.73005,     0.72921,      0.7294,     0.72959,     0.72889,     0.72868,     0.72885,     0.72902,     0.72909,     0.72857,     0.72795,     0.72596,     0.72526,     0.72421,     0.72449,     0.72396,     0.72381,     0.72404,     0.72362,      0.7232,     0.72292,     0.72266,\n",
       "            0.72239,     0.72106,     0.71935,     0.71983,        0.72,     0.71873,     0.71802,      0.7173,     0.71268,     0.71138,     0.70859,     0.70823,     0.70787,     0.70739,     0.70667,     0.70594,     0.70466,     0.70415,     0.70384,     0.70353,     0.70125,     0.70015,     0.69942,\n",
       "            0.69741,     0.69673,     0.69636,       0.696,     0.69515,     0.69429,     0.69256,     0.69224,     0.69192,     0.69161,     0.69022,     0.68948,     0.68842,      0.6873,     0.68652,     0.68467,     0.68393,     0.67895,      0.6767,     0.67523,     0.67348,     0.67152,     0.66984,\n",
       "            0.66976,      0.6693,     0.66736,     0.66504,     0.66468,     0.66481,     0.66423,     0.66073,     0.65995,     0.65933,     0.65874,     0.65657,      0.6545,     0.65238,     0.65204,     0.65076,     0.64873,     0.64605,     0.64525,     0.64445,     0.64264,     0.63845,     0.63383,\n",
       "            0.63269,     0.63187,     0.63106,     0.62789,     0.62727,     0.62603,     0.62097,     0.61905,     0.61842,     0.61693,      0.6159,     0.61457,     0.61022,      0.6095,     0.61039,     0.60698,     0.60726,     0.60454,     0.60402,     0.60336,     0.60179,     0.59951,     0.59694,\n",
       "            0.59436,     0.59137,     0.58745,     0.58502,     0.58163,     0.58096,      0.5802,     0.57931,      0.5772,     0.56794,     0.56397,     0.56306,     0.56108,     0.55917,     0.55317,     0.55248,     0.55003,     0.54618,     0.54307,     0.54131,      0.5399,     0.53041,     0.52803,\n",
       "            0.52255,     0.52159,     0.51779,     0.51455,     0.51347,     0.51011,     0.50913,     0.50572,     0.50428,     0.50329,     0.49964,     0.49577,     0.48677,     0.48224,     0.47968,     0.47891,      0.4786,     0.47341,     0.47202,     0.46851,     0.46788,     0.46726,     0.46043,\n",
       "            0.45662,     0.45029,     0.44609,      0.4392,     0.43682,     0.43042,     0.42622,      0.4213,     0.41668,     0.41557,     0.41293,     0.40583,     0.39292,     0.39143,     0.39057,     0.38947,     0.38416,     0.38217,     0.37702,     0.37024,     0.35976,     0.35787,     0.35081,\n",
       "            0.33653,      0.3328,      0.3276,     0.32208,     0.31833,     0.30528,     0.29769,     0.28813,     0.27847,     0.27064,     0.25684,     0.25279,      0.2387,     0.23253,     0.22017,      0.2118,     0.19493,     0.18728,     0.18193,     0.17103,     0.16457,     0.16022,     0.15871,\n",
       "            0.15344,     0.15089,     0.13768,     0.13497,     0.11889,      0.1072,     0.10007,    0.095319,    0.092764,    0.078346,    0.058822,    0.055936,    0.053414,    0.046121,    0.036016,    0.032055,     0.02821,    0.026478,    0.012878,       0.012,    0.011122,   0.0097101,   0.0043479,\n",
       "          0.0030199,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.16742,     0.16742,      0.2305,     0.26843,     0.29666,     0.32187,     0.34148,     0.35734,     0.37201,      0.3867,     0.39479,     0.40679,     0.41589,     0.42326,     0.43089,     0.44136,      0.4503,     0.45901,     0.46521,     0.47064,     0.47718,     0.48213,     0.48836,\n",
       "            0.49416,     0.49836,     0.50462,     0.51099,     0.51545,     0.52042,     0.52286,      0.5268,     0.52752,     0.53165,     0.53611,     0.53903,     0.54246,     0.54691,     0.54948,     0.55288,      0.5553,     0.55629,     0.55812,     0.56051,     0.56332,     0.56633,     0.56754,\n",
       "            0.57207,     0.57508,     0.57724,     0.58298,     0.58618,     0.58969,     0.59214,     0.59437,     0.59833,     0.59989,     0.60272,     0.60373,     0.60662,     0.60757,     0.60934,     0.61289,     0.61328,     0.61566,     0.61873,     0.62098,      0.6246,     0.62681,     0.62891,\n",
       "            0.63038,      0.6307,     0.63148,     0.63309,     0.63341,     0.63591,     0.63769,     0.64104,     0.64256,     0.64302,     0.64406,      0.6458,     0.64928,     0.64991,     0.65152,     0.65359,     0.65679,     0.65755,     0.65817,     0.65948,     0.65983,     0.66097,     0.66399,\n",
       "            0.66479,     0.66522,     0.66562,     0.66681,     0.66869,     0.67116,      0.6732,     0.67523,     0.67551,     0.67847,     0.67982,     0.68097,     0.68284,     0.68333,     0.68396,      0.6854,     0.68522,     0.68696,     0.68796,      0.6883,     0.68855,     0.68881,     0.68961,\n",
       "            0.68965,     0.69158,     0.69291,     0.69272,     0.69333,      0.6945,     0.69668,     0.69678,     0.69621,     0.69793,      0.6999,     0.70093,     0.70138,     0.70268,     0.70373,     0.70534,     0.70631,     0.70666,     0.70743,      0.7082,     0.71063,     0.71243,      0.7128,\n",
       "            0.71269,     0.71257,     0.71408,     0.71391,     0.71417,     0.71478,     0.71756,     0.71879,      0.7199,     0.72086,     0.72153,     0.72202,     0.72192,     0.72181,     0.72195,     0.72301,     0.72467,     0.72682,     0.72875,     0.73083,     0.73124,     0.73164,     0.73234,\n",
       "            0.73281,     0.73476,     0.73553,     0.73669,     0.73708,     0.73899,     0.73938,     0.73977,     0.74088,     0.74292,     0.74277,     0.74328,     0.74477,     0.74701,     0.74802,     0.74854,     0.74894,     0.74934,     0.74926,     0.74916,     0.74954,     0.74993,      0.7496,\n",
       "            0.74948,     0.75098,     0.75215,     0.75238,     0.75261,     0.75284,     0.75337,     0.75436,     0.75559,     0.75563,     0.75556,      0.7555,     0.75543,     0.75659,     0.75733,      0.7579,     0.75912,     0.76026,      0.7615,     0.76225,     0.76378,     0.76487,     0.76465,\n",
       "            0.76533,     0.76698,     0.76986,     0.77131,     0.77202,     0.77273,     0.77306,     0.77291,     0.77319,      0.7748,     0.77509,     0.77538,     0.77567,     0.77614,     0.77663,     0.77853,     0.77826,     0.77812,     0.77823,      0.7786,     0.77896,     0.77941,     0.77988,\n",
       "            0.78098,     0.78171,     0.78285,     0.78446,     0.78552,     0.78571,     0.78639,     0.78693,     0.78741,     0.78801,     0.78855,      0.7907,     0.79134,     0.79116,     0.79203,     0.79238,     0.79272,      0.7933,     0.79419,     0.79513,     0.79557,     0.79584,     0.79566,\n",
       "            0.79677,     0.79756,     0.79829,     0.79832,     0.79917,     0.80021,     0.80128,      0.8012,     0.80112,     0.80105,      0.8012,     0.80135,     0.80151,     0.80166,     0.80181,     0.80197,     0.80215,     0.80243,     0.80272,     0.80301,     0.80442,     0.80485,     0.80541,\n",
       "            0.80686,     0.80717,     0.80809,     0.80788,     0.80783,     0.80777,     0.80772,     0.80766,     0.80758,     0.80748,      0.8072,     0.80764,     0.80808,     0.80865,     0.80927,      0.8091,     0.80874,     0.80868,     0.80863,     0.80858,     0.80853,     0.80949,     0.81018,\n",
       "            0.81189,     0.81253,     0.81251,     0.81236,     0.81225,     0.81215,     0.81232,     0.81296,     0.81306,      0.8131,       0.814,     0.81385,       0.814,     0.81445,     0.81485,     0.81471,     0.81518,     0.81613,     0.81678,     0.81896,     0.81887,     0.81876,     0.81862,\n",
       "             0.8188,     0.81926,     0.81965,     0.81949,     0.81995,     0.82041,     0.82135,     0.82157,     0.82176,     0.82195,     0.82215,     0.82234,     0.82253,     0.82292,     0.82334,      0.8236,     0.82452,     0.82423,     0.82468,     0.82559,     0.82626,     0.82674,     0.82717,\n",
       "            0.82753,     0.82747,     0.82741,     0.82735,     0.82727,     0.82716,     0.82684,     0.82752,     0.82805,      0.8283,     0.82855,      0.8288,     0.82905,     0.82891,     0.83004,     0.82997,     0.82988,     0.82995,     0.83038,     0.83082,     0.83149,     0.83236,     0.83305,\n",
       "            0.83287,     0.83376,     0.83371,     0.83366,      0.8336,      0.8341,     0.83548,       0.837,     0.83685,     0.83677,     0.83668,     0.83906,     0.84073,     0.84118,     0.84269,     0.84263,     0.84257,     0.84251,     0.84289,     0.84351,     0.84457,      0.8449,     0.84486,\n",
       "            0.84482,     0.84478,     0.84474,     0.84467,     0.84495,     0.84621,     0.84689,     0.84678,     0.84665,     0.84652,     0.84673,     0.84705,     0.84737,     0.84769,     0.84813,     0.84859,      0.8492,      0.8502,     0.85008,     0.85033,     0.85131,     0.85154,     0.85176,\n",
       "            0.85199,     0.85221,     0.85243,     0.85294,     0.85358,     0.85361,     0.85339,     0.85433,     0.85563,     0.85615,      0.8565,     0.85685,     0.85721,     0.85765,     0.85808,     0.85852,     0.85896,      0.8594,     0.85983,     0.86027,     0.86071,     0.86144,      0.8625,\n",
       "            0.86316,     0.86429,     0.86514,     0.86581,     0.86699,     0.86833,     0.86968,     0.86972,     0.87087,     0.87355,     0.87429,     0.87497,     0.87481,     0.87508,     0.87535,     0.87563,      0.8759,     0.87614,     0.87609,     0.87604,     0.87599,     0.87592,     0.87627,\n",
       "            0.87661,     0.87696,     0.87701,     0.87689,     0.87717,     0.87745,     0.87772,       0.878,     0.87833,     0.87868,     0.87902,     0.87937,     0.88055,     0.88195,     0.88395,     0.88466,     0.88465,     0.88456,      0.8842,     0.88429,     0.88457,     0.88486,     0.88514,\n",
       "            0.88546,     0.88617,     0.88671,     0.88743,      0.8894,     0.88934,     0.88928,     0.88905,     0.88899,     0.88893,     0.88883,     0.88897,     0.88969,     0.89213,      0.8929,     0.89289,      0.8931,     0.89331,     0.89352,     0.89373,     0.89394,     0.89415,     0.89412,\n",
       "            0.89425,     0.89455,     0.89484,     0.89514,     0.89543,     0.89545,      0.8954,     0.89534,     0.89532,      0.8953,     0.89529,     0.89527,     0.89525,     0.89524,     0.89522,     0.89521,     0.89519,     0.89518,     0.89677,     0.89816,      0.8989,     0.89948,     0.89932,\n",
       "            0.90063,     0.90055,     0.90047,     0.90038,     0.90031,     0.90027,     0.90023,     0.90019,     0.89993,      0.8998,     0.89971,      0.8997,     0.90008,     0.90046,     0.90084,     0.90089,     0.90072,     0.90165,     0.90188,     0.90237,     0.90314,       0.906,     0.90629,\n",
       "            0.90621,     0.90613,     0.90605,     0.90749,     0.90741,     0.90837,     0.90891,     0.90886,     0.90881,      0.9092,     0.90999,     0.90993,     0.91082,     0.91191,     0.91271,     0.91287,     0.91394,     0.91436,     0.91432,     0.91428,     0.91424,      0.9142,     0.91415,\n",
       "            0.91409,     0.91431,     0.91456,     0.91481,     0.91505,      0.9153,     0.91555,     0.91551,      0.9155,     0.91549,     0.91548,     0.91546,     0.91545,     0.91544,     0.91543,     0.91542,     0.91541,      0.9154,     0.91538,     0.91537,     0.91678,     0.91792,     0.91853,\n",
       "            0.91849,     0.91846,     0.91842,     0.91838,     0.91833,     0.91828,     0.91823,     0.91818,     0.91813,     0.91832,     0.91862,     0.91892,     0.91922,     0.91951,     0.91968,     0.91953,     0.91973,     0.92056,      0.9224,     0.92305,     0.92347,     0.92389,     0.92432,\n",
       "            0.92594,     0.92631,     0.92669,     0.92706,     0.92743,     0.92748,     0.92739,     0.92732,     0.92726,     0.92719,     0.92707,     0.92701,     0.92732,     0.92763,     0.92794,     0.92825,     0.92855,     0.92845,     0.92832,     0.92987,     0.93073,     0.93152,     0.93328,\n",
       "            0.93501,     0.93559,     0.93617,     0.93635,     0.93633,     0.93631,     0.93628,     0.93626,     0.93623,     0.93619,     0.93614,     0.93603,     0.93614,     0.93641,     0.93668,     0.93695,     0.93722,     0.93749,     0.93934,      0.9393,     0.93926,     0.93921,     0.93896,\n",
       "            0.93874,     0.93868,     0.93985,     0.94084,     0.94156,     0.94192,     0.94185,     0.94235,     0.94296,     0.94356,     0.94359,     0.94357,     0.94355,     0.94352,      0.9435,     0.94339,     0.94327,     0.94312,     0.94305,     0.94302,     0.94299,     0.94655,     0.94648,\n",
       "            0.94642,     0.94637,      0.9463,     0.94624,     0.94619,     0.94653,     0.94778,     0.94768,     0.94756,     0.94753,      0.9475,     0.94747,     0.94735,     0.94724,     0.94718,     0.94714,      0.9471,     0.94707,     0.94694,     0.94691,     0.94689,     0.94687,     0.94684,\n",
       "            0.94679,     0.94675,      0.9466,      0.9464,     0.94633,     0.94615,     0.94605,     0.94594,     0.94585,     0.94577,      0.9468,     0.94754,     0.94746,     0.94733,     0.94719,     0.94704,     0.94698,     0.94692,     0.94685,     0.94677,     0.94643,      0.9463,     0.94625,\n",
       "            0.94621,     0.94617,     0.94669,     0.94737,     0.94802,     0.94795,     0.94824,     0.94882,      0.9494,     0.94989,     0.94983,     0.94977,     0.94956,     0.94949,     0.94991,     0.95136,     0.95131,     0.95197,     0.95328,     0.95324,     0.95319,     0.95317,     0.95314,\n",
       "            0.95312,     0.95299,      0.9539,     0.95562,     0.95688,     0.95676,      0.9567,     0.95664,     0.95622,      0.9561,     0.95584,     0.95581,     0.95578,     0.95573,     0.95566,      0.9556,     0.95548,     0.95543,      0.9554,     0.95537,     0.95516,     0.95505,     0.95499,\n",
       "             0.9548,     0.95473,      0.9547,     0.95466,     0.95458,      0.9545,     0.95433,      0.9543,     0.95427,     0.95424,     0.95411,     0.95403,     0.95393,     0.95382,     0.95375,     0.95356,     0.95349,       0.953,     0.95497,     0.95483,     0.95497,     0.95671,     0.95796,\n",
       "            0.95881,     0.95877,      0.9586,     0.95839,      0.9595,     0.96067,     0.96063,     0.96267,      0.9626,     0.96255,      0.9625,     0.96233,     0.96216,     0.96198,     0.96314,     0.96425,     0.96409,     0.96388,     0.96382,     0.96375,     0.96361,     0.96412,     0.96541,\n",
       "            0.96532,     0.96526,     0.96519,     0.96495,      0.9649,      0.9648,      0.9644,     0.96424,     0.96419,      0.9667,     0.96662,     0.96919,     0.96888,     0.97441,     0.97988,     0.98056,     0.98202,     0.98244,     0.98242,     0.98239,     0.98233,     0.98224,     0.98213,\n",
       "            0.98202,     0.98189,     0.98172,     0.98162,     0.98147,     0.98144,     0.98141,     0.98137,      0.9843,     0.98397,     0.98382,     0.98378,      0.9837,     0.98363,     0.98338,     0.98336,     0.98326,      0.9831,     0.98296,     0.98289,     0.98283,     0.98242,     0.98231,\n",
       "            0.98206,     0.98202,     0.98184,     0.98169,     0.98164,     0.98148,     0.98143,     0.98127,      0.9812,     0.98115,     0.98097,     0.98188,     0.98417,     0.98397,     0.98386,     0.98383,     0.98598,     0.98762,     0.99167,     0.99159,     0.99158,     0.99156,      0.9914,\n",
       "            0.99131,     0.99115,     0.99104,     0.99087,      0.9908,     0.99063,     0.99051,     0.99037,     0.99024,     0.99021,     0.99013,     0.98991,      0.9895,     0.98945,     0.98942,     0.98939,     0.98921,     0.98914,     0.98896,     0.98871,     0.98831,     0.98824,     0.98795,\n",
       "            0.98734,     0.98717,     0.98788,     0.99328,     0.99319,     0.99284,     0.99263,     0.99234,     0.99204,     0.99177,     0.99126,      0.9911,     0.99051,     0.99022,     0.98961,     0.98915,     0.98812,     0.98759,     0.98719,      0.9863,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.96229,     0.96229,     0.95319,     0.94668,     0.94018,     0.93498,     0.92848,     0.92479,     0.92198,     0.91938,     0.91417,     0.91417,     0.91287,     0.91157,     0.91157,     0.91027,     0.90767,     0.90695,     0.90507,     0.90117,     0.89727,      0.8971,     0.89597,\n",
       "            0.89467,     0.89467,     0.89467,     0.89413,     0.89337,     0.89326,     0.89207,     0.88887,     0.88557,     0.88427,     0.88427,     0.88427,     0.88166,     0.88166,     0.88036,     0.87906,     0.87906,     0.87776,     0.87646,     0.87516,     0.87386,     0.87386,     0.87256,\n",
       "            0.87256,     0.87256,     0.87256,     0.87256,     0.87126,     0.87126,     0.87126,     0.87126,     0.87126,     0.86996,     0.86996,     0.86996,     0.86866,     0.86866,     0.86866,     0.86736,     0.86614,     0.86606,     0.86606,     0.86606,     0.86606,     0.86606,     0.86606,\n",
       "            0.86476,     0.86476,     0.86216,     0.86161,     0.86086,     0.86086,     0.86059,     0.85956,     0.85956,     0.85956,     0.85885,     0.85826,     0.85826,     0.85697,     0.85696,     0.85696,     0.85696,     0.85566,     0.85566,     0.85566,     0.85508,     0.85436,     0.85306,\n",
       "            0.85306,     0.85306,     0.85306,     0.85306,     0.85176,     0.85176,     0.85176,     0.85176,     0.85046,     0.85046,     0.85046,     0.85046,     0.85046,     0.85046,     0.85046,     0.84992,     0.84923,     0.84915,     0.84915,     0.84915,     0.84915,     0.84915,     0.84785,\n",
       "            0.84668,     0.84561,     0.84504,     0.84395,     0.84395,     0.84395,     0.84395,     0.84267,      0.8404,     0.84005,     0.84005,     0.84005,     0.84005,     0.83875,     0.83875,     0.83875,     0.83875,     0.83745,     0.83745,     0.83745,     0.83745,     0.83615,      0.8359,\n",
       "            0.83544,     0.83497,     0.83467,     0.83396,     0.83225,     0.83103,     0.83095,     0.83095,     0.83095,     0.83095,     0.83095,      0.8309,     0.83046,     0.83002,     0.82965,     0.82965,     0.82965,     0.82965,     0.82965,     0.82965,     0.82965,     0.82965,     0.82899,\n",
       "            0.82835,     0.82835,     0.82835,     0.82835,     0.82835,     0.82835,     0.82835,     0.82835,     0.82705,     0.82675,     0.82575,     0.82575,     0.82445,     0.82445,     0.82445,     0.82445,     0.82445,     0.82445,     0.82381,     0.82315,     0.82315,     0.82315,     0.82142,\n",
       "            0.82088,     0.82055,     0.82055,     0.82055,     0.82055,     0.82055,     0.82055,     0.82055,     0.82055,     0.82028,     0.81999,      0.8197,     0.81941,     0.81925,     0.81925,     0.81825,     0.81795,     0.81795,     0.81664,     0.81664,     0.81664,      0.8164,     0.81543,\n",
       "            0.81534,     0.81534,     0.81534,     0.81534,     0.81534,     0.81534,     0.81505,     0.81438,     0.81404,     0.81404,     0.81404,     0.81404,     0.81404,     0.81404,     0.81404,     0.81367,     0.81239,     0.81176,     0.81144,     0.81144,     0.81144,     0.81144,     0.81144,\n",
       "            0.81144,     0.81144,     0.81144,     0.81144,     0.81144,     0.81014,     0.81014,     0.81014,     0.81014,     0.81014,     0.80884,     0.80884,     0.80879,     0.80791,     0.80624,     0.80624,     0.80624,     0.80624,     0.80624,     0.80624,     0.80624,       0.806,     0.80511,\n",
       "            0.80494,     0.80432,     0.80364,     0.80234,     0.80234,     0.80234,     0.80225,     0.80184,     0.80143,     0.80104,     0.80104,     0.80104,     0.80104,     0.80104,     0.80104,     0.80104,     0.80104,     0.80104,     0.80104,     0.80104,     0.80104,     0.80104,     0.80104,\n",
       "            0.80104,     0.80015,     0.79974,     0.79838,      0.7981,     0.79781,     0.79753,     0.79725,     0.79682,     0.79629,     0.79454,     0.79454,     0.79454,     0.79454,     0.79453,     0.79364,     0.79179,     0.79153,     0.79126,     0.79099,     0.79073,     0.79064,     0.79064,\n",
       "            0.79064,     0.79064,     0.78894,     0.78818,      0.7876,     0.78707,     0.78674,     0.78674,     0.78614,     0.78544,     0.78544,     0.78456,     0.78414,     0.78414,     0.78407,     0.78331,     0.78283,     0.78283,     0.78283,     0.78238,      0.7819,     0.78134,     0.78058,\n",
       "            0.78023,     0.78023,     0.78009,     0.77893,     0.77893,     0.77893,     0.77893,     0.77763,     0.77763,     0.77763,     0.77763,     0.77763,     0.77763,     0.77763,     0.77763,     0.77716,     0.77597,      0.7744,     0.77373,     0.77373,     0.77373,     0.77373,     0.77373,\n",
       "            0.77369,     0.77336,     0.77302,     0.77269,      0.7723,      0.7717,     0.76983,     0.76983,     0.76983,     0.76983,     0.76983,     0.76983,     0.76983,      0.7686,     0.76853,     0.76805,     0.76757,     0.76723,     0.76723,     0.76723,     0.76593,     0.76593,     0.76593,\n",
       "            0.76468,     0.76306,     0.76278,      0.7625,     0.76222,     0.76203,     0.76203,      0.7612,     0.76042,     0.75993,     0.75945,     0.75943,     0.75943,     0.75943,     0.75932,     0.75897,     0.75861,     0.75825,     0.75813,     0.75813,     0.75813,     0.75796,     0.75773,\n",
       "             0.7575,     0.75727,     0.75703,     0.75664,     0.75553,     0.75553,     0.75524,     0.75458,     0.75387,      0.7531,     0.75293,     0.75293,     0.75293,     0.75293,     0.75293,     0.75293,     0.75293,     0.75279,     0.75212,     0.75163,     0.75163,     0.75163,     0.75163,\n",
       "            0.75163,     0.75163,     0.75163,     0.75163,     0.75163,     0.75067,     0.74934,     0.74902,     0.74902,     0.74902,     0.74902,     0.74902,     0.74902,     0.74902,     0.74902,     0.74902,     0.74902,     0.74902,     0.74902,     0.74902,     0.74902,     0.74772,     0.74772,\n",
       "            0.74772,     0.74772,     0.74772,     0.74772,     0.74772,     0.74772,     0.74772,      0.7466,     0.74642,     0.74642,     0.74642,     0.74642,     0.74514,     0.74512,     0.74512,     0.74512,     0.74512,     0.74508,     0.74473,     0.74437,     0.74402,     0.74252,     0.74252,\n",
       "            0.74252,     0.74252,     0.74179,     0.73992,     0.73992,     0.73992,     0.73992,     0.73992,     0.73992,     0.73992,     0.73992,     0.73992,     0.73992,     0.73992,     0.73862,     0.73862,     0.73803,     0.73736,     0.73477,     0.73342,     0.73342,     0.73342,     0.73342,\n",
       "            0.73342,     0.73342,     0.73212,     0.73212,     0.73204,     0.73159,     0.73115,     0.72941,     0.72896,     0.72852,     0.72778,     0.72692,     0.72692,     0.72692,     0.72635,     0.72562,     0.72562,     0.72562,     0.72562,     0.72562,     0.72562,     0.72562,     0.72479,\n",
       "            0.72432,     0.72432,     0.72432,     0.72432,     0.72432,     0.72398,     0.72353,     0.72309,     0.72291,     0.72279,     0.72267,     0.72255,     0.72243,     0.72231,     0.72219,     0.72207,     0.72195,     0.72182,     0.72172,     0.72172,     0.72172,     0.72147,     0.72014,\n",
       "            0.71896,     0.71829,     0.71763,     0.71696,      0.7164,     0.71607,     0.71574,     0.71541,     0.71334,     0.71231,     0.71165,     0.71001,     0.71001,     0.71001,     0.71001,     0.70922,     0.70788,     0.70741,     0.70522,     0.70481,     0.70481,     0.70481,      0.7043,\n",
       "            0.70363,     0.70297,      0.7023,     0.70163,     0.70097,     0.70091,     0.70071,     0.70026,     0.69982,     0.69961,     0.69961,     0.69624,     0.69571,     0.69571,     0.69571,     0.69481,     0.69441,     0.69417,     0.69383,      0.6935,     0.69317,     0.69274,      0.6923,\n",
       "            0.69186,     0.69181,     0.69181,     0.69181,     0.69181,     0.69181,     0.69181,     0.69045,     0.69035,     0.69025,     0.69014,     0.69004,     0.68994,     0.68984,     0.68973,     0.68963,     0.68953,     0.68943,     0.68932,     0.68922,     0.68921,     0.68921,     0.68906,\n",
       "            0.68873,      0.6884,     0.68806,     0.68767,     0.68723,     0.68678,     0.68634,      0.6859,     0.68545,     0.68531,     0.68531,     0.68531,     0.68531,     0.68531,      0.6849,     0.68356,      0.6827,      0.6827,      0.6827,      0.6827,      0.6827,      0.6827,      0.6827,\n",
       "             0.6827,      0.6827,      0.6827,      0.6827,      0.6827,     0.68184,     0.68096,     0.68029,     0.67962,     0.67896,     0.67778,      0.6762,      0.6762,      0.6762,      0.6762,      0.6762,      0.6762,     0.67495,     0.67361,      0.6723,      0.6723,     0.67222,      0.6697,\n",
       "             0.6697,      0.6697,      0.6697,     0.66954,      0.6693,     0.66905,     0.66881,     0.66857,     0.66824,      0.6677,     0.66717,     0.66594,      0.6645,      0.6645,      0.6645,      0.6645,      0.6645,      0.6645,     0.66447,     0.66403,     0.66358,     0.66301,     0.66016,\n",
       "            0.65755,     0.65688,      0.6554,      0.6554,      0.6554,     0.65382,     0.65293,      0.6528,      0.6528,      0.6528,     0.65255,     0.65228,     0.65202,     0.65175,     0.65144,     0.65011,     0.64866,     0.64679,     0.64606,     0.64567,     0.64529,     0.64481,     0.64392,\n",
       "            0.64319,     0.64253,     0.64168,     0.64087,      0.6402,     0.63849,     0.63849,     0.63597,     0.63442,     0.63404,     0.63366,     0.63323,     0.63181,      0.6303,     0.62964,     0.62911,     0.62867,     0.62822,     0.62658,     0.62628,     0.62599,     0.62569,     0.62532,\n",
       "            0.62479,     0.62426,     0.62238,     0.61992,     0.61904,     0.61685,     0.61574,     0.61441,     0.61331,     0.61231,     0.61118,     0.61065,      0.6097,     0.60815,     0.60638,     0.60454,     0.60387,     0.60315,     0.60226,     0.60137,     0.59729,     0.59575,     0.59519,\n",
       "            0.59475,      0.5943,     0.59298,     0.59298,     0.59296,     0.59207,     0.59168,     0.59168,     0.59168,     0.59158,     0.59092,     0.59013,     0.58759,      0.5867,     0.58518,     0.58498,     0.58432,     0.58388,     0.58368,     0.58315,     0.58262,     0.58227,     0.58193,\n",
       "             0.5816,     0.57992,     0.57737,     0.57737,     0.57713,     0.57554,     0.57465,     0.57376,     0.56802,      0.5664,     0.56296,     0.56252,     0.56207,     0.56149,      0.5606,     0.55971,     0.55814,     0.55753,     0.55714,     0.55676,     0.55398,     0.55265,     0.55176,\n",
       "            0.54932,     0.54851,     0.54806,     0.54762,     0.54659,     0.54556,     0.54348,      0.5431,     0.54272,     0.54234,     0.54069,      0.5398,     0.53853,      0.5372,     0.53626,     0.53407,     0.53319,     0.52731,     0.52401,     0.52228,     0.52016,     0.51731,     0.51495,\n",
       "            0.51462,     0.51409,     0.51185,     0.50918,     0.50845,     0.50828,     0.50761,     0.50297,     0.50208,     0.50138,     0.50072,     0.49826,     0.49593,     0.49354,     0.49285,      0.4911,     0.48883,     0.48585,     0.48496,     0.48407,     0.48207,     0.47724,     0.47179,\n",
       "            0.47055,     0.46966,     0.46877,     0.46535,     0.46468,     0.46334,     0.45791,     0.45585,     0.45519,     0.45302,     0.45192,     0.44995,     0.44536,     0.44343,     0.44325,     0.43953,     0.43953,     0.43659,     0.43606,     0.43538,     0.43375,     0.43141,     0.42878,\n",
       "            0.42614,     0.42309,     0.41913,     0.41667,     0.41327,      0.4126,     0.41184,     0.41095,     0.40832,     0.39916,     0.39528,     0.39439,     0.39247,     0.39061,     0.38482,     0.38415,      0.3818,     0.37813,     0.37517,     0.37351,     0.37218,     0.36327,     0.36106,\n",
       "            0.35598,      0.3551,     0.35161,     0.34864,     0.34766,     0.34461,     0.34372,     0.34064,     0.33934,     0.33845,     0.33518,      0.3316,     0.32335,     0.31938,     0.31715,     0.31648,     0.31599,     0.31132,     0.30972,     0.30671,     0.30618,     0.30565,     0.29984,\n",
       "            0.29663,     0.29132,     0.28782,     0.28213,     0.28017,     0.27494,     0.27153,     0.26756,     0.26385,     0.26296,     0.26086,     0.25524,     0.24513,     0.24397,      0.2433,     0.24246,     0.23837,     0.23684,      0.2329,     0.22777,      0.2199,      0.2185,     0.21327,\n",
       "            0.20283,     0.20014,     0.19636,      0.1922,     0.18954,     0.18037,      0.1751,     0.16854,     0.16197,      0.1567,     0.14753,     0.14487,      0.1357,     0.13173,     0.12387,      0.1186,     0.10813,     0.10345,      0.1002,    0.093632,    0.089666,    0.087084,    0.086195,\n",
       "           0.083097,    0.081603,    0.073929,     0.07237,    0.063202,    0.056635,    0.052669,    0.050044,    0.048638,     0.04077,    0.030302,    0.028773,     0.02744,    0.023605,    0.018338,    0.016289,    0.014307,    0.013417,   0.0064807,   0.0060364,   0.0055921,   0.0048787,   0.0021787,\n",
       "          0.0015122,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
       "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: 0.6749371619061687\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.65445])\n",
       "names: {0: 'person'}\n",
       "nt_per_class: array([769], dtype=int64)\n",
       "nt_per_image: array([200], dtype=int64)\n",
       "results_dict: {'metrics/precision(B)': 0.8842009360690894, 'metrics/recall(B)': 0.7347698804135735, 'metrics/mAP50(B)': 0.859309035736446, 'metrics/mAP50-95(B)': 0.654451398147249, 'fitness': 0.6749371619061687}\n",
       "save_dir: WindowsPath('runs/detect/train14')\n",
       "speed: {'preprocess': 0.44213099929038435, 'inference': 2.9869725002208725, 'loss': 0.0017419998766854405, 'postprocess': 4.563580999383703}\n",
       "stats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO('yolov8n.pt')  # O yolov8s.pt, yolov8m.pt\n",
    "\n",
    "model.train(\n",
    "    data=r'day2_files\\dataset.yaml',\n",
    "    imgsz=640,\n",
    "    epochs=100,\n",
    "    batch=16,\n",
    "    device=0  # O 'cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yourk\\.conda\\envs\\r_foam\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input, Dense "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the data to train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset \n",
    "(x_train, _), (x_test, _) = mnist.load_data() \n",
    "\n",
    "# Normalize the pixel values \n",
    "x_train = x_train.astype('float32') / 255. \n",
    "x_test = x_test.astype('float32') / 255. \n",
    "\n",
    "# Flatten the images \n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))) \n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the structure of the model \n",
    "\n",
    "As you can see this model is quite similar to the previus one that we create.\n",
    "\n",
    "This model use the method of *TRANSFORM* which use a encoder, decoder and bottleneck, however they are still hidden layers with relu activations. This \n",
    "Type of application is a *autoencoder*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"day2_files\\images\\autoencoders.png\" alt=\"Neural Network General\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yourk\\.conda\\envs\\r_foam\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\yourk\\.conda\\envs\\r_foam\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 784)               50960     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105392 (411.69 KB)\n",
      "Trainable params: 105392 (411.69 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Encoder \n",
    "input_layer = Input(shape=(784,)) \n",
    "encoded = Dense(64, activation='relu')(input_layer) \n",
    "\n",
    "# Bottleneck \n",
    "bottleneck = Dense(32, activation='relu')(encoded) \n",
    "\n",
    "# Decoder \n",
    "decoded = Dense(64, activation='relu')(bottleneck) \n",
    "output_layer = Dense(784, activation='sigmoid')(decoded) \n",
    "\n",
    "# Autoencoder model \n",
    "autoencoder = Model(input_layer, output_layer) \n",
    "\n",
    "# Compile the model \n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy') \n",
    "\n",
    "# Summary of the model \n",
    "autoencoder.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:From c:\\Users\\yourk\\.conda\\envs\\r_foam\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "235/235 [==============================] - 4s 8ms/step - loss: 0.2552 - val_loss: 0.1720\n",
      "Epoch 2/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1548 - val_loss: 0.1392\n",
      "Epoch 3/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1337 - val_loss: 0.1266\n",
      "Epoch 4/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1238 - val_loss: 0.1181\n",
      "Epoch 5/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1167 - val_loss: 0.1123\n",
      "Epoch 6/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1116 - val_loss: 0.1079\n",
      "Epoch 7/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1079 - val_loss: 0.1047\n",
      "Epoch 8/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1050 - val_loss: 0.1024\n",
      "Epoch 9/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1029 - val_loss: 0.1004\n",
      "Epoch 10/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1011 - val_loss: 0.0990\n",
      "Epoch 11/25\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0997 - val_loss: 0.0979\n",
      "Epoch 12/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0987 - val_loss: 0.0969\n",
      "Epoch 13/25\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.0978 - val_loss: 0.0960\n",
      "Epoch 14/25\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.0970 - val_loss: 0.0959\n",
      "Epoch 15/25\n",
      "235/235 [==============================] - 3s 11ms/step - loss: 0.0963 - val_loss: 0.0948\n",
      "Epoch 16/25\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.0956 - val_loss: 0.0939\n",
      "Epoch 17/25\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.0949 - val_loss: 0.0935\n",
      "Epoch 18/25\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0943 - val_loss: 0.0929\n",
      "Epoch 19/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0937 - val_loss: 0.0925\n",
      "Epoch 20/25\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0932 - val_loss: 0.0918\n",
      "Epoch 21/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0927 - val_loss: 0.0915\n",
      "Epoch 22/25\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0924 - val_loss: 0.0910\n",
      "Epoch 23/25\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0920 - val_loss: 0.0907\n",
      "Epoch 24/25\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0917 - val_loss: 0.0905\n",
      "Epoch 25/25\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.0914 - val_loss: 0.0902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x242802055d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(\n",
    "    x_train, x_train,  \n",
    "    epochs=25,  \n",
    "    batch_size=256,  \n",
    "    shuffle=True,  \n",
    "    validation_data=(x_test, x_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results \n",
    "\n",
    "As you can appreciate the unsupervised learning one of the function is making generating images, learning of their own the patrons and objectives to achive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Predict the test data \n",
    "reconstructed = autoencoder.predict(x_test) \n",
    "\n",
    "\n",
    "n = 10  # Number of digits to display \n",
    "plt.figure(figsize=(20, 4)) \n",
    "\n",
    "for i in range(n): \n",
    "    # Display original \n",
    "    ax = plt.subplot(2, n, i + 1) \n",
    "    plt.imshow(x_test[i].reshape(28, 28)) \n",
    "    plt.gray() \n",
    "    ax.get_xaxis().set_visible(False) \n",
    "    ax.get_yaxis().set_visible(False) \n",
    "\n",
    "    # Display reconstruction \n",
    "    ax = plt.subplot(2, n, i + 1 + n) \n",
    "    plt.imshow(reconstructed[i].reshape(28, 28)) \n",
    "    plt.gray() \n",
    "    ax.get_xaxis().set_visible(False) \n",
    "    ax.get_yaxis().set_visible(False) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 4 Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now creating the a customatizable model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU to the rescue \n",
    "\n",
    "GPU as showed before it is highly important to obtain, to reduced significally the waiting time but also ensure a good performance.\n",
    "It is required to have a GPU  activated or be in Collab in order to work with customizable models specially in images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿CUDA disponible?: True\n",
      "Dispositivo actual: 0\n",
      "Nombre del dispositivo: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Número de GPUs disponibles: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"¿CUDA disponible?:\", torch.cuda.is_available())\n",
    "print(\"Dispositivo actual:\", torch.cuda.current_device())\n",
    "print(\"Nombre del dispositivo:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "print(\"Número de GPUs disponibles:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data \n",
    "\n",
    "Configure the data and downloading the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model \n",
    "\n",
    "A simple configuration of a VGG16 that already exist but this case the model those the following :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"day2_files\\model.png\" alt=\"Neural Network General\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVGG16(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*56*56, 128), nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mVGG16Complete\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# CIFAR-10 tiene 10 clases\u001b[39;00m\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(VGG16Complete, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class VGG16Complete(nn.Module):\n",
    "    def __init__(self, num_classes=10):  # CIFAR-10 tiene 10 clases\n",
    "        super(VGG16Complete, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Bloque 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Bloque 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Bloque 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Bloque 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Bloque 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 7 * 7, 4096), nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 4096), nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = VGG16Complete(num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train Loss: 0.3763 - Train Acc: 0.9194\n",
      "Epoch 2/3 - Train Loss: 0.1053 - Train Acc: 0.9677\n",
      "Epoch 3/3 - Train Loss: 0.0695 - Train Acc: 0.9783\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleVGG16(num_classes=10).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 3  # rápido para ejemplo\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleVGG16(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=200704, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardar\n",
    "torch.save(model.state_dict(), \"modelo_vgg16.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleVGG16(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=200704, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleVGG16(num_classes=10).to(device)\n",
    "model.load_state_dict(torch.load(\"modelo_vgg16.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL EXAM (For the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload and tranform the image before evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yourk\\.conda\\envs\\r_foam\\lib\\site-packages\\PIL\\Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "image_path = r\"day2_files\\R.png\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),       \n",
    "    transforms.ToTensor(),              \n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],  \n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "input_tensor = transform(image).unsqueeze(0).to(device) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción del modelo: clase 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "\n",
    "print(f\"Predicción del modelo: clase {pred_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the probabilities\n",
    "\n",
    "This is the way after the model generate the weight and bias, and output behaviour of the neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades por clase:\n",
      "Clase 0: 0.0034\n",
      "Clase 1: 0.0001\n",
      "Clase 2: 0.0026\n",
      "Clase 3: 0.0057\n",
      "Clase 4: 0.0001\n",
      "Clase 5: 0.0000\n",
      "Clase 6: 0.0000\n",
      "Clase 7: 0.9833\n",
      "Clase 8: 0.0015\n",
      "Clase 9: 0.0033\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "probs = F.softmax(output, dim=1)\n",
    "print(\"Probabilities per classes:\")\n",
    "for i, p in enumerate(probs[0]):\n",
    "    print(f\"Clase {i}: {p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the validation batch again for reasuring the precision of the model in general, although it is recommend to use another batch external to the data downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of the model: 97.35%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Precision of the model: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHALLENGE !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reached the end of the workshop! Now it's your turn to create your own project.\n",
    "Please divide into groups and design a model based on a goal SDG of your choice.\n",
    "The only limitations are the following:\n",
    "\n",
    " * Your project must include a trained model (YOLOv8, PyTorch, or any framework you prefer).\n",
    " * Using segmentation, live camera input, or complex datasets will earn extra points.\n",
    " * It is recommended to use Kaggle datasets, as they often include pre-processed images and labels.\n",
    " * The model should achieve at least 75% accuracy — higher is better, but make sure to avoid overfitting. \n",
    " * At the end of training, you must test and validate your model as demonstrated in the workshop. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r_foam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
